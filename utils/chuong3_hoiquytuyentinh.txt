import numpy as np
from sklearn.linear_model import LinearRegression

A = np.array([[40,1,30,1.1],
              [60,2,32,1.55],
              [53,2,30.1,1.68],
              [71,2,35.7,1.75],
              [80,2,24.5,5.5],
              [56,2,27.6,2.3],
              [75,2,27.6,3],
              [79,2,27.6,3.5],
              [56,2,29.7,2.4],
              [60,2,29.7,2.9],
              [72,2,29.7,3],
              [95,3,29.7,4.2],
              [47,1,19.3,1.5],
              [91,2,18.1,2.2],
              [68,1,21.4,1.5],
              [69,2,17.5,3.15],
              [82,2,25.1,3.4],
              [60,2,26.5,2.245],
              [68,2,26.5,2.4]])

X = A[:,:-1]
y = A[:,-1].reshape(-1,1)
X = np.concatenate((np.ones((X.shape[0],1)), X),axis=1)

# a
W = []
a = X.T @ X
w = (np.linalg.pinv(a) @ X.T @ y).reshape(1,-1)
W.append(w)
print(f'a) w = {w}')


def compute_gradient(X, y, w):
    m = X.shape[0]
    z = X @ w.T
    dw = X.T @ ((-2/m)*(y-z))
    return dw

def gradient_descent(X, y, learning_rate, iterations, w_in):
    m = X.shape[0]
    w = w_in
    for i in range(iterations):
        w_new = w - learning_rate * compute_gradient(X, y, w).reshape(1,-1)
        if abs(np.linalg.norm(compute_gradient(X, y, w_new), 2)) < 1e-3:
            print(f'w = {w_new}, iteration = {i}')
            return w_new
        w = w_new
    print(f'Failed to obtain the minimum of the function after {iterations} iterations')

def accelerated_gradient_descent(X, y, learning_rate, iterations, w_in):
    previous_w, current_w = np.copy(w_in), w_in
    for i in range(iterations):
        t = current_w + ((i - 1) / (i + 2)) * (current_w - previous_w)
        w_new = t - learning_rate * compute_gradient(X, y, t).reshape(1,-1)
        if abs(np.linalg.norm(compute_gradient(X, y, w_new), 2)) < 1e-3:
            print(f'w = {w_new}, iteration = {i}')
            return w_new
        previous_w = current_w
        current_w = w_new
    print(f'Failed to obtain the minimum of the function after {iterations} iterations')

def stochastic_gradient_descent(X, y, learning_rate, iterations, w_in):
    m = X.shape[0]
    w = w_in
    for i in range(iterations):
        i_rand = np.random.randint(m)
        xi, yi = X[i_rand].reshape(1,-1), y[i_rand]
        w_new = w - learning_rate * compute_gradient(xi, yi, w).reshape(1,-1)
        if np.linalg.norm(compute_gradient(X, y, w_new)) < 1e-2:
            print(f'w = {w_new}, iteration = {i}')
            return w_new
        w = w_new
    print(f'Failed to obtain the minimum of the function after {iterations} iterations')

print('b)')
n = X.shape[1]
w_in = np.zeros((1,n))

w = gradient_descent(X, y, learning_rate=0.0001, iterations=600000, w_in=w_in)
W.append(w)

w = accelerated_gradient_descent(X, y, learning_rate=0.0001, iterations=600000, w_in=w_in)
W.append(w)

# Do stochastic là lấy random nên có thể chạy lần đầu không hội tụ, nên cô chạy nhiều lần nha cô!
w_in = np.array([[0,0,0.9,0]])
w = stochastic_gradient_descent(X, y, learning_rate=0.0001, iterations=1000000, w_in=w_in)
W.append(w)

x_test = np.array([[1,79,2,26.5]])
preds = []
for w in W:
    preds.append((x_test @ w.T).item())

# Scikit learn Linear Regression
model = LinearRegression(fit_intercept=False)
model.fit(X,y)
model.coef_
preds.append(model.predict(x_test).item())

algorithm = ['Derivative','GD', 'Accelerated GD', 'Stochastic GD', 'Scikit-Learn']
for i, pred in enumerate(preds):
    print(f'{algorithm[i]} predict {pred}, Error = {np.linalg.norm(pred-2.5)}')